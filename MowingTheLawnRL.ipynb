{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+6R9kYbcfNOSvzB9MFbJc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Luke-687/Portfolio/blob/main/MowingTheLawnRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KJO4ZqKcGxg"
      },
      "outputs": [],
      "source": [
        "#Idea: Make a RL model which can teach itself whether or not to mow the lawn, so think +3 for successfully mowing, 0 for not mowing, -5 partly mowing\n",
        "rewardMow = 5\n",
        "rewardNoMow = -4\n",
        "rewardPartMow = -11\n",
        "\n",
        "#States: based on two parameters, battery (high or low) and rain (high >=50% or low<50%)\n",
        "states = [\n",
        "    #[battery, rain]\n",
        "    (\"high\", \"high\"),\n",
        "    (\"high\", \"low\"),\n",
        "    (\"low\", \"high\"),\n",
        "    (\"low\", \"low\")\n",
        "]\n",
        "\n",
        "#Actions: include all possible actions from each\n",
        "actions = {\n",
        "    (\"high\", \"high\"): [\"mow\", \"noMow\"],\n",
        "    (\"high\", \"low\"): [\"mow\", \"noMow\"],\n",
        "    (\"low\", \"high\"): [\"mow\", \"noMow\"],\n",
        "    (\"low\", \"low\"): [\"mow\", \"noMow\"]\n",
        "}\n",
        "\n",
        "#Transitions, map out possibilities(high to low for rain means it rained)\n",
        "transitions = {\n",
        "  ((\"high\", \"high\"), \"mow\"): [((\"high\", \"high\"), 0.5, rewardMow), ((\"high\", \"low\"), 0.2, rewardPartMow), ((\"low\",  \"high\"), 0.2, rewardMow), ((\"low\", \"low\"), 0.1, rewardPartMow)],\n",
        "  ((\"high\", \"high\"), \"noMow\"): [((\"high\", \"high\"), 0.5, rewardNoMow), ((\"high\", \"low\"), 0.5, rewardNoMow)],\n",
        "\n",
        "  ((\"high\", \"low\"), \"mow\"): [((\"high\", \"low\"), 0.8, rewardMow), ((\"low\",  \"low\"), 0.2, rewardMow)],\n",
        "  ((\"high\", \"low\"), \"noMow\"): [((\"high\", \"low\"), 0.7, rewardNoMow), ((\"high\", \"high\"), 0.2, rewardNoMow)],\n",
        "\n",
        "  ((\"low\", \"high\"), \"mow\"): [((\"low\", \"high\"), 0.5, rewardMow), ((\"high\", \"low\"), 0.05, rewardPartMow), ((\"low\",  \"low\"), 0.1, rewardPartMow), ((\"high\", \"high\"), 0.15, rewardPartMow)],\n",
        "  ((\"low\", \"high\"), \"noMow\"): [((\"low\", \"high\"), 0.5, rewardNoMow), ((\"low\", \"low\"), 0.5, rewardNoMow)],\n",
        "\n",
        "  ((\"low\", \"low\"), \"mow\"): [((\"high\", \"high\"), 0.1, rewardPartMow), ((\"high\", \"low\"), 0.2, rewardPartMow), ((\"low\",  \"high\"), 0.2, rewardMow), ((\"low\", \"low\"), 0.5, rewardMow)],\n",
        "  ((\"low\", \"low\"), \"noMow\"): [((\"low\", \"low\"), 0.9, rewardNoMow), ((\"low\", \"high\"), 0.1, rewardNoMow)],\n",
        "}\n",
        "\n",
        "def trainer(states, actions, transitions, episodes, gamma = 0.99, theta = 0.001):\n",
        "  V = {s: 0 for s in states}\n",
        "  policy = {s: None for s in states}\n",
        "\n",
        "  for i in range (0, episodes):\n",
        "    delta = 0\n",
        "    for s in states:\n",
        "      actVals = {}\n",
        "      for a in actions[s]:\n",
        "        tot = 0\n",
        "        for (s_next, prob, reward) in transitions[(s, a)]:\n",
        "          tot += prob * (reward + gamma * V[s_next])\n",
        "        actVals[a] = tot\n",
        "      maxAct = max(actVals, key = actVals.get)\n",
        "      delta = max(delta, abs(V[s] - actVals[maxAct]))\n",
        "      V[s] = actVals[maxAct]\n",
        "      policy[s] = maxAct\n",
        "    if(delta<=theta):\n",
        "      break\n",
        "\n",
        "  return V,policy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "V,policy = trainer(states, actions, transitions, 100)\n",
        "policy[(\"low\",\"high\")]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oT1xrDaql-Wh",
        "outputId": "da8569ad-8a43-496c-9808-95a984065304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mow'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}